{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will be using the Half Cheetah environment in the MuJoCo benchmark, where the goal is the control this cheetah to run forward as fast and steady as possible!\n"
      ],
      "metadata": {
        "id": "NZoVYL7ArIsO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPR6Hm1v0dkH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526fd2b8-ae1d-4d99-81a3-6f5b8941b7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title mount your Google Drive\n",
        "# @markdown Your work will be stored in a folder called `Imitation_Learning` by default to prevent Colab instance timeouts from deleting your edits.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0Jq_S5T0ebX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543acc4f-25cb-424b-e775-2c603123cd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Imitation_Learning\n"
          ]
        }
      ],
      "source": [
        "#@title set up mount symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/Imitation_Learning'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/Imitation_Learning'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH\n",
        "\n",
        "%cd $SYM_PATH\n",
        "\n",
        "## save the HalfCheetah_expert_data.pkl file in this directory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Mujoco Library:"
      ],
      "metadata": {
        "id": "qd5zSxiH5IMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline\n",
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "id": "MGC8F-M0qUjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed95402f-decf-4c77-b439-1851f2f9201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Expert Data"
      ],
      "metadata": {
        "id": "SF0nCvR15kLP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xXqwQcF1SJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e46af06-e7bd-40c0-9ebd-44075ad1370a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['observation', 'action', 'reward', 'next_observation', 'terminal'])\n",
            "number of data: 1000\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from typing import Optional, Tuple, Union\n",
        "from gymnasium import logger, spaces\n",
        "\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Loading data\n",
        "file_path = \"HalfCheetah_expert_data.pkl\"\n",
        "with open(file_path, \"rb\") as f:\n",
        "    expert_data = pickle.load(f)[0]\n",
        "\n",
        "print(expert_data.keys())\n",
        "print(\"number of data:\", len(expert_data['observation']))\n",
        "\n",
        "# Extract expert states and actions\n",
        "states = torch.tensor(expert_data[\"observation\"], dtype=torch.float32)\n",
        "actions = torch.tensor(expert_data[\"action\"], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the network, environment and evaluation function."
      ],
      "metadata": {
        "id": "3KnXrCW35rvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtOQByK60cAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8b3f74-46f2-4416-8787-5df401c64360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: Mean Reward = -5.32, Std Reward = 0.50\n"
          ]
        }
      ],
      "source": [
        "# Define a simple neural network policy for Behavior Cloning\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize policy network\n",
        "env = gym.make(\"HalfCheetah-v5\")\n",
        "state_dim = states.shape[1]\n",
        "action_dim = actions.shape[1]\n",
        "policy_bc = PolicyNet(state_dim, action_dim)\n",
        "\n",
        "# Define the evaluate_policy function\n",
        "def evaluate_policy(policy, env, episodes=10):\n",
        "    total_rewards = []\n",
        "    for _ in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        for i in range(1000):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            action = policy(state_tensor).detach().numpy()[0]\n",
        "            state, reward, done, _, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            if done:\n",
        "              break\n",
        "        total_rewards.append(episode_reward)\n",
        "    print(f\"Evaluation Results: Mean Reward = {np.mean(total_rewards):.2f}, Std Reward = {np.std(total_rewards):.2f}\")\n",
        "    return np.mean(total_rewards), np.std(total_rewards)\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(policy_bc, env)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "\n",
        "criterion = nn.MSELoss() #Using MSE for loss\n",
        "optimizer = torch.optim.Adam(policy_bc.parameters(), lr=1e-3)\n",
        "\n",
        "# Implement the Behavior Cloning Algorithm here\n",
        "\n",
        "#Creating our dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "dataset = TensorDataset(states, actions)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "#Training\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for batch_states, batch_actions in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predicted_actions = policy_bc(batch_states)\n",
        "        loss = criterion(predicted_actions, batch_actions)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_states.size(0)\n",
        "\n",
        "    # Print training statistics\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Evaluate and save the trained BC policy\n",
        "mean_reward, std_reward = evaluate_policy(policy_bc, env)\n",
        "policy_path = \"bc_policy.pth\"\n",
        "torch.save(policy_bc.state_dict(), policy_path)\n",
        "print(f\"Trained policy saved at {policy_path}\")"
      ],
      "metadata": {
        "id": "WcRbCJB70szr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924f4b6c-ee6f-40bc-e475-19286ac8f332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 0.4486\n",
            "Epoch [2/1000], Loss: 0.2097\n",
            "Epoch [3/1000], Loss: 0.1373\n",
            "Epoch [4/1000], Loss: 0.1129\n",
            "Epoch [5/1000], Loss: 0.0984\n",
            "Epoch [6/1000], Loss: 0.0896\n",
            "Epoch [7/1000], Loss: 0.0811\n",
            "Epoch [8/1000], Loss: 0.0756\n",
            "Epoch [9/1000], Loss: 0.0708\n",
            "Epoch [10/1000], Loss: 0.0667\n",
            "...\n",
            "Epoch [991/1000], Loss: 0.0013\n",
            "Epoch [992/1000], Loss: 0.0013\n",
            "Epoch [993/1000], Loss: 0.0012\n",
            "Epoch [994/1000], Loss: 0.0013\n",
            "Epoch [995/1000], Loss: 0.0012\n",
            "Epoch [996/1000], Loss: 0.0012\n",
            "Epoch [997/1000], Loss: 0.0012\n",
            "Epoch [998/1000], Loss: 0.0013\n",
            "Epoch [999/1000], Loss: 0.0012\n",
            "Epoch [1000/1000], Loss: 0.0013\n",
            "Evaluation Results: Mean Reward = 4082.30, Std Reward = 100.31\n",
            "Trained policy saved at bc_policy.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize policy\n",
        "policy_dagger = PolicyNet(state_dim, action_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(policy_dagger.parameters(), lr=1e-3)\n",
        "\n",
        "# Implement the Dagger Algorithm here\n",
        "\n",
        "\n",
        "# DAgger parameters\n",
        "dagger_iterations = 5\n",
        "epochs_per_iteration = 100\n",
        "rollout_episodes = 10\n",
        "max_steps = 2000 #Defining maximum steps to avoid infinite loops on rollouts\n",
        "\n",
        "# Initialize aggregated dataset\n",
        "aggregated_states = states.clone()\n",
        "aggregated_actions = actions.clone()\n",
        "\n",
        "\n",
        "for dagger_iter in range(dagger_iterations):\n",
        "    print(f\"\\nDAgger Iteration {dagger_iter+1}/{dagger_iterations}\")\n",
        "    #Train on current aggregated dataset\n",
        "    dataset = TensorDataset(aggregated_states, aggregated_actions)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs_per_iteration):\n",
        "        total_loss = 0.0\n",
        "        for batch_states, batch_actions in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            pred_actions = policy_dagger(batch_states)\n",
        "            loss = criterion(pred_actions, batch_actions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * batch_states.size(0)\n",
        "\n",
        "        # Print epoch statistics every 5 epochs\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            avg_loss = loss / len(dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs_per_iteration} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Collecting rollouts...\")\n",
        "    #Collecting new trajectories and expert annotations\n",
        "    new_states, new_expert_actions = [], []\n",
        "    for _ in range(rollout_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        steps = 0  #step counter\n",
        "        while not done and steps < max_steps:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "                action_tensor = policy_dagger(state_tensor)\n",
        "                action = action_tensor.squeeze(0).numpy()  # Convert to numpy after computation\n",
        "\n",
        "            #Store state and get expert action\n",
        "            new_states.append(state)\n",
        "            expert_idx = torch.randint(0, len(actions), (1,))\n",
        "            expert_action = actions[expert_idx].squeeze(0).numpy()\n",
        "            new_expert_actions.append(expert_action)\n",
        "\n",
        "            #Environment step\n",
        "            state, _, done, _, _ = env.step(action)\n",
        "            steps += 1\n",
        "\n",
        "    #Aggregate new data\n",
        "    new_states_tensor = torch.tensor(np.array(new_states), dtype=torch.float32)\n",
        "    new_actions_tensor = torch.tensor(np.array(new_expert_actions), dtype=torch.float32)\n",
        "\n",
        "    aggregated_states = torch.cat([aggregated_states, new_states_tensor])\n",
        "    aggregated_actions = torch.cat([aggregated_actions, new_actions_tensor])\n",
        "\n",
        "# Save trained policy\n",
        "mean_reward, std_reward = evaluate_policy(policy_dagger, env)\n",
        "policy_path = \"dagger_policy.pth\"\n",
        "torch.save(policy_dagger.state_dict(), policy_path)\n",
        "print(f\"Trained policy saved at {policy_path}\")\n"
      ],
      "metadata": {
        "id": "fqLXYL5sj5Jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4951a706-dabd-4487-8d3a-208ff4b255bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DAgger Iteration 1/5\n",
            "Epoch 5/100 | Loss: 0.0064\n",
            "Epoch 10/100 | Loss: 0.0043\n",
            "...\n",
            "Epoch 95/100 | Loss: 0.0005\n",
            "Epoch 100/100 | Loss: 0.0005\n",
            "Collecting rollouts...\n",
            "\n",
            "DAgger Iteration 2/5\n",
            "Epoch 5/100 | Loss: 0.0021\n",
            "Epoch 10/100 | Loss: 0.0018\n",
            "...\n",
            "Epoch 95/100 | Loss: 0.0021\n",
            "Epoch 100/100 | Loss: 0.0011\n",
            "Collecting rollouts...\n",
            "\n",
            "DAgger Iteration 3/5\n",
            "Epoch 5/100 | Loss: 0.0009\n",
            "Epoch 10/100 | Loss: 0.0008\n",
            "...\n",
            "Epoch 95/100 | Loss: 0.0008\n",
            "Epoch 100/100 | Loss: 0.0008\n",
            "Collecting rollouts...\n",
            "\n",
            "DAgger Iteration 4/5\n",
            "Epoch 5/100 | Loss: 0.0004\n",
            "Epoch 10/100 | Loss: 0.0005\n",
            "...\n",
            "Epoch 95/100 | Loss: 0.0005\n",
            "Epoch 100/100 | Loss: 0.0006\n",
            "Collecting rollouts...\n",
            "\n",
            "DAgger Iteration 5/5\n",
            "Epoch 5/100 | Loss: 0.0004\n",
            "Epoch 10/100 | Loss: 0.0004\n",
            "...\n",
            "Epoch 95/100 | Loss: 0.0004\n",
            "Epoch 100/100 | Loss: 0.0004\n",
            "Collecting rollouts...\n",
            "Evaluation Results: Mean Reward = -26.10, Std Reward = 1.14\n",
            "Trained policy saved at dagger_policy.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esUbNTVV6Y7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
